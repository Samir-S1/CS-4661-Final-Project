{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23b4d186",
   "metadata": {},
   "source": [
    "# Financial Stock Price Prediction with LSTM\n",
    "\n",
    "This notebook demonstrates how to use LSTM neural networks to predict stock prices using historical financial data from Yahoo Finance. We'll adapt the LSTM architecture for time series regression and financial forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0561963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device available: {'GPU' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4123d7b4",
   "metadata": {},
   "source": [
    "## Data Collection with yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b2b26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download stock data using yfinance\n",
    "ticker_symbol = \"AAPL\"  # Apple Inc. stock\n",
    "start_date = \"2020-01-01\"\n",
    "end_date = \"2024-12-01\"\n",
    "\n",
    "print(f\"Downloading {ticker_symbol} stock data from {start_date} to {end_date}...\")\n",
    "\n",
    "# Download the data\n",
    "stock_data = yf.download(ticker_symbol, start=start_date, end=end_date)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(f\"\\nDataset shape: {stock_data.shape}\")\n",
    "print(f\"Date range: {stock_data.index[0]} to {stock_data.index[-1]}\")\n",
    "print(f\"Columns: {list(stock_data.columns)}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst 5 rows of the data:\")\n",
    "print(stock_data.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"\\nMissing values per column:\")\n",
    "print(stock_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc54a640",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc9ebc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for preprocessing\n",
    "df = stock_data.copy()\n",
    "\n",
    "# Use closing price as our main target variable\n",
    "df['Close_Price'] = df['Close']\n",
    "\n",
    "# Create additional features\n",
    "df['Price_Change'] = df['Close'].pct_change()\n",
    "df['Volume_MA_10'] = df['Volume'].rolling(window=10).mean()\n",
    "df['High_Low_Pct'] = (df['High'] - df['Low']) / df['Close'] * 100\n",
    "df['Open_Close_Pct'] = (df['Close'] - df['Open']) / df['Open'] * 100\n",
    "\n",
    "# Create moving averages\n",
    "df['MA_5'] = df['Close'].rolling(window=5).mean()\n",
    "df['MA_10'] = df['Close'].rolling(window=10).mean()\n",
    "df['MA_20'] = df['Close'].rolling(window=20).mean()\n",
    "\n",
    "# Create volatility measure (rolling standard deviation)\n",
    "df['Volatility'] = df['Price_Change'].rolling(window=10).std()\n",
    "\n",
    "# Drop rows with NaN values\n",
    "df = df.dropna()\n",
    "\n",
    "# Select features for training\n",
    "feature_columns = ['Close_Price', 'Volume', 'High_Low_Pct', 'Open_Close_Pct', \n",
    "                   'MA_5', 'MA_10', 'MA_20', 'Volatility']\n",
    "df_features = df[feature_columns].copy()\n",
    "\n",
    "print(f\"Shape after preprocessing: {df_features.shape}\")\n",
    "print(f\"Features selected: {feature_columns}\")\n",
    "print(\"\\nProcessed data sample:\")\n",
    "print(df_features.head())\n",
    "\n",
    "# Visualize the closing price\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df.index, df['Close_Price'])\n",
    "plt.title(f'{ticker_symbol} Stock Price Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Close Price ($)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd93d03",
   "metadata": {},
   "source": [
    "## Create Dataset Class for Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae8be92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data using MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(df_features.values)\n",
    "\n",
    "print(f\"Data shape after scaling: {scaled_data.shape}\")\n",
    "print(f\"Sample scaled data (first 3 rows):\")\n",
    "print(scaled_data[:3])\n",
    "\n",
    "class StockDataset(Dataset):\n",
    "    def __init__(self, data, sequence_length, target_column=0):\n",
    "        self.data = data\n",
    "        self.sequence_length = sequence_length\n",
    "        self.target_column = target_column\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.sequence_length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get sequence of features\n",
    "        sequence = self.data[idx:idx + self.sequence_length]\n",
    "        # Get target (next closing price)\n",
    "        target = self.data[idx + self.sequence_length, self.target_column]\n",
    "        \n",
    "        return torch.FloatTensor(sequence), torch.FloatTensor([target])\n",
    "\n",
    "# Split data into train/test sets (80/20 split)\n",
    "train_size = int(len(scaled_data) * 0.8)\n",
    "train_data = scaled_data[:train_size]\n",
    "test_data = scaled_data[train_size:]\n",
    "\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Test data shape: {test_data.shape}\")\n",
    "\n",
    "# Set sequence length (number of days to look back)\n",
    "sequence_length = 60\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = StockDataset(train_data, sequence_length)\n",
    "test_dataset = StockDataset(test_data, sequence_length)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "\n",
    "# Test the dataset\n",
    "sample_seq, sample_target = train_dataset[0]\n",
    "print(f\"Sample sequence shape: {sample_seq.shape}\")\n",
    "print(f\"Sample target shape: {sample_target.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d060511",
   "metadata": {},
   "source": [
    "## Define LSTM Model for Financial Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e420ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinancialLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size=1, dropout=0.2):\n",
    "        super(FinancialLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, \n",
    "                           batch_first=True, dropout=dropout)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Output layer\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        lstm_out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Get the last time step output\n",
    "        last_output = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Apply dropout\n",
    "        last_output = self.dropout(last_output)\n",
    "        \n",
    "        # Apply linear layer\n",
    "        predictions = self.linear(last_output)\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "# Model parameters\n",
    "input_size = len(feature_columns)  # Number of features\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "output_size = 1  # Predicting one value (closing price)\n",
    "\n",
    "# Create model instance\n",
    "model = FinancialLSTM(input_size, hidden_size, num_layers, output_size)\n",
    "\n",
    "# Print model architecture\n",
    "print(\"Model Architecture:\")\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa7707c",
   "metadata": {},
   "source": [
    "## Set Hyperparameters and Prepare Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26debceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', \n",
    "                                                 factor=0.5, patience=10, verbose=True)\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Learning rate: {learning_rate}\")\n",
    "print(f\"Number of epochs: {num_epochs}\")\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a91de2",
   "metadata": {},
   "source": [
    "## Training Loop Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c195eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_model(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_idx, (sequences, targets) in enumerate(train_loader):\n",
    "        sequences = sequences.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(sequences)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "# Validation function\n",
    "def validate_model(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sequences, targets in test_loader:\n",
    "            sequences = sequences.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(test_loader)\n",
    "\n",
    "# Training loop\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train the model\n",
    "    train_loss = train_model(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Validate the model\n",
    "    val_loss = validate_model(model, test_loader, criterion, device)\n",
    "    \n",
    "    # Store losses\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Print progress\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}')\n",
    "\n",
    "print(\"Training completed!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd04c0de",
   "metadata": {},
   "source": [
    "## Model Evaluation and Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efef0454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set\n",
    "model.eval()\n",
    "predictions = []\n",
    "actuals = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sequences, targets in test_loader:\n",
    "        sequences = sequences.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        outputs = model(sequences)\n",
    "        predictions.extend(outputs.cpu().numpy())\n",
    "        actuals.extend(targets.cpu().numpy())\n",
    "\n",
    "# Convert to numpy arrays\n",
    "predictions = np.array(predictions).reshape(-1, 1)\n",
    "actuals = np.array(actuals).reshape(-1, 1)\n",
    "\n",
    "# Inverse transform to get actual price values\n",
    "# Create dummy array for inverse transform (we only need the first column)\n",
    "dummy_pred = np.zeros((len(predictions), len(feature_columns)))\n",
    "dummy_actual = np.zeros((len(actuals), len(feature_columns)))\n",
    "\n",
    "dummy_pred[:, 0] = predictions.flatten()\n",
    "dummy_actual[:, 0] = actuals.flatten()\n",
    "\n",
    "# Inverse transform\n",
    "pred_prices = scaler.inverse_transform(dummy_pred)[:, 0]\n",
    "actual_prices = scaler.inverse_transform(dummy_actual)[:, 0]\n",
    "\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(actual_prices, pred_prices)\n",
    "mae = mean_absolute_error(actual_prices, pred_prices)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# Calculate percentage accuracy\n",
    "mape = np.mean(np.abs((actual_prices - pred_prices) / actual_prices)) * 100\n",
    "\n",
    "print(\"Model Performance Metrics:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"Mean Absolute Percentage Error (MAPE): {mape:.2f}%\")\n",
    "print(f\"Accuracy: {100 - mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b169081",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f600e532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Training and Validation Loss\n",
    "axes[0, 0].plot(train_losses, label='Training Loss', color='blue')\n",
    "axes[0, 0].plot(val_losses, label='Validation Loss', color='red')\n",
    "axes[0, 0].set_title('Training and Validation Loss')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# 2. Actual vs Predicted Prices (last 100 points for clarity)\n",
    "last_n = min(100, len(actual_prices))\n",
    "axes[0, 1].plot(actual_prices[-last_n:], label='Actual', color='green', linewidth=2)\n",
    "axes[0, 1].plot(pred_prices[-last_n:], label='Predicted', color='orange', linewidth=2)\n",
    "axes[0, 1].set_title(f'Actual vs Predicted Prices (Last {last_n} days)')\n",
    "axes[0, 1].set_xlabel('Days')\n",
    "axes[0, 1].set_ylabel('Price ($)')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# 3. Scatter plot of Actual vs Predicted\n",
    "axes[1, 0].scatter(actual_prices, pred_prices, alpha=0.5, color='purple')\n",
    "axes[1, 0].plot([actual_prices.min(), actual_prices.max()], \n",
    "                [actual_prices.min(), actual_prices.max()], \n",
    "                'r--', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Actual Prices ($)')\n",
    "axes[1, 0].set_ylabel('Predicted Prices ($)')\n",
    "axes[1, 0].set_title('Actual vs Predicted Prices Scatter Plot')\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# 4. Prediction Error Distribution\n",
    "errors = actual_prices - pred_prices\n",
    "axes[1, 1].hist(errors, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[1, 1].set_xlabel('Prediction Error ($)')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('Distribution of Prediction Errors')\n",
    "axes[1, 1].axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nPrediction Summary:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total test samples: {len(actual_prices)}\")\n",
    "print(f\"Actual price range: ${actual_prices.min():.2f} - ${actual_prices.max():.2f}\")\n",
    "print(f\"Predicted price range: ${pred_prices.min():.2f} - ${pred_prices.max():.2f}\")\n",
    "print(f\"Average actual price: ${actual_prices.mean():.2f}\")\n",
    "print(f\"Average predicted price: ${pred_prices.mean():.2f}\")\n",
    "print(f\"Standard deviation of errors: ${errors.std():.2f}\")\n",
    "\n",
    "# Show last few predictions\n",
    "print(f\"\\nLast 5 predictions vs actual:\")\n",
    "print(\"Date\\t\\tActual\\t\\tPredicted\\tError\")\n",
    "print(\"-\" * 50)\n",
    "for i in range(-5, 0):\n",
    "    error = actual_prices[i] - pred_prices[i]\n",
    "    print(f\"Day {len(actual_prices)+i}\\t${actual_prices[i]:.2f}\\t\\t${pred_prices[i]:.2f}\\t\\t{error:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
