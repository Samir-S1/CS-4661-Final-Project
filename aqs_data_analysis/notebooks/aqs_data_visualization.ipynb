{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# AQS Data Import\n",
    "\n",
    "Simple notebook to import AQS data from CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Load AQS data from CSV\n",
    "df = pd.read_csv('../data/hourly_88101_2020.csv')\n",
    "\n",
    "print(f\"Data loaded successfully!\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {len(df.columns)}\")\n",
    "#remove duplicates\n",
    "df = df.drop_duplicates()\n",
    "print(f\"Shape after removing duplicates: {df.shape}\")\n",
    "print(f\"Columns after removing duplicates: {len(df.columns)}\") \n",
    "\n",
    "#Uncap display\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "\n",
    "# Display first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rows where sample value is greater than 100\n",
    "df[df['Sample Measurement'] > 100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to essential columns\n",
    "# Drop rows if 'Site Num' is not 10\n",
    "print(df.columns.tolist())\n",
    "df = df.drop(df[df[\"Site Num\"] != 10].index)\n",
    "\n",
    "df = df[['Date GMT', 'Time GMT', 'Sample Measurement']]\n",
    "\n",
    "# Sort by date then time\n",
    "df = df.sort_values(['Date GMT', 'Time GMT'])\n",
    "\n",
    "# Reset index after sorting\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Check the results\n",
    "print(f\"Shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "datetime_gmt = pd.to_datetime(df['Date GMT'] + ' ' + df['Time GMT'])\n",
    "plt.plot(datetime_gmt, df['Sample Measurement'], marker='o', linestyle='-', markersize=2)\n",
    "plt.title('Sample Measurement Over Time (GMT)')\n",
    "plt.xlabel('Date and Time (GMT)')\n",
    "plt.ylabel('Sample Measurement')\n",
    "plt.grid()\n",
    "plt.show()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive plot with infinite zoom using Plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Create datetime column\n",
    "df['datetime_gmt'] = pd.to_datetime(df['Date GMT'] + ' ' + df['Time GMT'])\n",
    "\n",
    "# Create interactive plot\n",
    "fig = px.line(df, \n",
    "              x='datetime_gmt', \n",
    "              y='Sample Measurement',\n",
    "              title='PM2.5 Measurements Over Time (Interactive)',\n",
    "              labels={'datetime_gmt': 'Date and Time (GMT)', \n",
    "                     'Sample Measurement': 'PM2.5 (µg/m³)'},\n",
    "              hover_data=['Date GMT', 'Time GMT'])\n",
    "\n",
    "# Customize the plot\n",
    "fig.update_traces(line=dict(width=1))\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Date and Time (GMT)\",\n",
    "    yaxis_title=\"PM2.5 (µg/m³)\",\n",
    "    hovermode='x unified'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we want to normalize the sample measurement column\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "df['Sample Measurement Normalized'] = scaler.fit_transform(df[['Sample Measurement']])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datetime column\n",
    "df['datetime_gmt'] = pd.to_datetime(df['Date GMT'] + ' ' + df['Time GMT'])\n",
    "\n",
    "# Create interactive plot\n",
    "fig = px.line(df, \n",
    "              x='datetime_gmt', \n",
    "              y='Sample Measurement Normalized',\n",
    "              title='PM2.5 Measurements Over Time (Interactive)',\n",
    "              labels={'datetime_gmt': 'Date and Time (GMT)', \n",
    "                     'Sample Measurement': 'PM2.5 (µg/m³)'},\n",
    "              hover_data=['Date GMT', 'Time GMT'])\n",
    "\n",
    "# Customize the plot\n",
    "fig.update_traces(line=dict(width=1))\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Date and Time (GMT)\",\n",
    "    yaxis_title=\"PM2.5 (µg/m³)\",\n",
    "    hovermode='x unified'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive AQS Data Analysis - Python/Pandas Version\n",
    "print(\"\\n=== DATASET OVERVIEW ===\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "print(f\"Date range: {df['Date GMT'].min()} to {df['Date GMT'].max()}\")\n",
    "\n",
    "print(\"\\n=== COLUMN ANALYSIS ===\")\n",
    "for col in df.columns:\n",
    "    print(f\"{col}: {df[col].dtype}, {df[col].nunique():,} unique values\")\n",
    "\n",
    "print(\"\\n=== PM2.5 MEASUREMENT STATISTICS ===\")\n",
    "stats = df['Sample Measurement'].describe()\n",
    "print(stats)\n",
    "print(f\"Missing values: {df['Sample Measurement'].isna().sum():,}\")\n",
    "\n",
    "# State analysis (if 'State Name' exists)\n",
    "if 'State Name' in df.columns:\n",
    "    state_stats = df.groupby('State Name').size().reset_index(name='Count')\n",
    "    print(\"\\n=== STATE ANALYSIS ===\")\n",
    "    print(state_stats.sort_values('Count', ascending=False).head(10))\n",
    "else:\n",
    "    print(\"\\n=== STATE ANALYSIS ===\")\n",
    "    print(\"Column 'State Name' not found in df.\")\n",
    "\n",
    "# High pollution episodes (>35 µg/m³ - EPA standard)\n",
    "high_pollution = df[df['Sample Measurement'] > 35]\n",
    "print(\"\\n=== HIGH POLLUTION EPISODES ===\")\n",
    "print(f\"Records above 35 µg/m³: {len(high_pollution):,} ({len(high_pollution)/len(df)*100:.2f}%)\")\n",
    "if len(high_pollution) > 0:\n",
    "    top_readings = high_pollution.nlargest(10, 'Sample Measurement')\n",
    "    print(\"\\nTop 10 highest readings:\")\n",
    "    print(top_readings[['Date GMT', 'Time GMT', 'Sample Measurement']])\n",
    "\n",
    "# Site analysis (if 'Site Num' exists)\n",
    "if 'Site Num' in df.columns:\n",
    "    site_counts = df.groupby('Site Num').size().reset_index(name='Records')\n",
    "    print(\"\\n=== MONITORING SITE ANALYSIS ===\")\n",
    "    print(f\"Total unique sites: {len(site_counts):,}\")\n",
    "    print(\"\\nMost active sites:\")\n",
    "    print(site_counts.nlargest(10, 'Records'))\n",
    "else:\n",
    "    print(\"\\n=== MONITORING SITE ANALYSIS ===\")\n",
    "    print(\"Column 'Site Num' not found in df.\")\n",
    "\n",
    "# Temporal patterns\n",
    "if 'datetime_gmt' in df.columns:\n",
    "    monthly_avg = df.groupby(df['datetime_gmt'].dt.to_period('M'))['Sample Measurement'].mean()\n",
    "    hourly_avg = df.groupby(df['datetime_gmt'].dt.hour)['Sample Measurement'].mean()\n",
    "    print(\"\\n=== TEMPORAL PATTERNS ===\")\n",
    "    print(\"Monthly averages:\")\n",
    "    print(monthly_avg)\n",
    "    print(\"\\nHourly averages:\")\n",
    "    print(hourly_avg)\n",
    "else:\n",
    "    print(\"\\n=== TEMPORAL PATTERNS ===\")\n",
    "    print(\"Column 'datetime_gmt' not found in df.\")\n",
    "\n",
    "print(\"\\n=== DATA QUALITY ASSESSMENT ===\")\n",
    "print(f\"Duplicate rows: {df.duplicated().sum():,}\")\n",
    "print(f\"Negative measurements: {(df['Sample Measurement'] < 0).sum():,}\")\n",
    "print(f\"Zero measurements: {(df['Sample Measurement'] == 0).sum():,}\")\n",
    "print(f\"Extremely high measurements (>500): {(df['Sample Measurement'] > 500).sum():,}\")\n",
    "\n",
    "# Coordinate system analysis (if columns exist)\n",
    "coord_cols = [c for c in df.columns if 'Latitude' in c or 'Longitude' in c or 'Datum' in c]\n",
    "if coord_cols:\n",
    "    coord_systems = df[coord_cols].drop_duplicates()\n",
    "    print(\"\\n=== COORDINATE SYSTEMS ===\")\n",
    "    print(\"Coordinate systems in use:\")\n",
    "    print(coord_systems)\n",
    "else:\n",
    "    print(\"\\n=== COORDINATE SYSTEMS ===\")\n",
    "    print(\"No coordinate system columns found.\")\n",
    "\n",
    "print(\"\\n=== ANALYSIS COMPLETE ===\")\n",
    "print(\"Variables available for further analysis:\")\n",
    "print(\"- df: Complete dataset\")\n",
    "if 'state_stats' in locals(): print(\"- state_stats: State-level statistics\")\n",
    "print(\"- high_pollution: Records above EPA standard\")\n",
    "if 'monthly_avg' in locals(): print(\"- monthly_avg: Monthly pollution averages\")\n",
    "if 'hourly_avg' in locals(): print(\"- hourly_avg: Hourly pollution averages\")\n",
    "if 'site_counts' in locals(): print(\"- site_counts: Site activity summary\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the SQLite database in the data folder and list all tables\n",
    "from pathlib import Path\n",
    "import sqlite3\n",
    "\n",
    "\n",
    "db_path = Path('../data/aqs_data.db')\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Query to list all tables\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "tables = cursor.fetchall()\n",
    "print(\"Tables in aqs_data.db:\")\n",
    "for table in tables:\n",
    "    print(table[0])\n",
    "\n",
    "cursor.close()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Define database path\n",
    "db_path = Path('../data/aqs_data.db')\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# Example query: select first 5 rows from the main table\n",
    "query = 'SELECT * FROM hourly_88101_2020 LIMIT 5'\n",
    "\n",
    "# Load query results into a pandas DataFrame\n",
    "df_demo = pd.read_sql_query(query, conn)\n",
    "\n",
    "# Show the DataFrame\n",
    "print(df_demo)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to database\n",
    "db_path = Path('../data/aqs_data.db')\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# First, let's get basic info about the table WITHOUT loading all data\n",
    "print(\"=== TABLE OVERVIEW ===\")\n",
    "\n",
    "# Get row count\n",
    "count_query = \"SELECT COUNT(*) FROM hourly_88101_2020\"\n",
    "total_rows = pd.read_sql_query(count_query, conn).iloc[0, 0]\n",
    "print(f\"Total rows in database: {total_rows:,}\")\n",
    "\n",
    "# Get column info\n",
    "pragma_query = \"PRAGMA table_info(hourly_88101_2020)\"\n",
    "columns_info = pd.read_sql_query(pragma_query, conn)\n",
    "print(f\"\\nColumns ({len(columns_info)}):\")\n",
    "for _, row in columns_info.iterrows():\n",
    "    print(f\"  {row['name']} ({row['type']})\")\n",
    "\n",
    "# Get sample of data (first 1000 rows)\n",
    "print(\"\\n=== SAMPLE DATA (First 1000 rows) ===\")\n",
    "sample_query = \"\"\"\n",
    "SELECT * FROM hourly_88101_2020 \n",
    "ORDER BY \"Date GMT\", \"Time GMT\"\n",
    "LIMIT 1000\n",
    "\"\"\"\n",
    "df_sample = pd.read_sql_query(sample_query, conn)\n",
    "print(f\"Sample shape: {df_sample.shape}\")\n",
    "print(f\"Date range in sample: {df_sample['Date GMT'].min()} to {df_sample['Date GMT'].max()}\")\n",
    "\n",
    "# # Show basic stats on Sample Measurement without loading all data\n",
    "# print(\"\\n=== SAMPLE MEASUREMENT STATISTICS ===\")\n",
    "# stats_query = \"\"\"\n",
    "# SELECT \n",
    "#     COUNT(*) as count,\n",
    "#     AVG(\"Sample Measurement\") as mean,\n",
    "#     MIN(\"Sample Measurement\") as min,\n",
    "#     MAX(\"Sample Measurement\") as max,\n",
    "#     COUNT(CASE WHEN \"Sample Measurement\" > 35 THEN 1 END) as above_epa_standard\n",
    "# FROM hourly_88101_2020\n",
    "# \"\"\"\n",
    "# stats = pd.read_sql_query(stats_query, conn)\n",
    "# print(stats)\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\nFirst 10 rows:\")\n",
    "df_sample.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smart querying approach for large datasets\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Connect to database\n",
    "db_path = Path('../data/aqs_data.db')\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# 1. RANDOM SAMPLE for exploration (fast)\n",
    "print(\"=== RANDOM SAMPLE (10,000 records) ===\")\n",
    "sample_query = \"\"\"\n",
    "SELECT * FROM hourly_88101_2020 \n",
    "ORDER BY RANDOM() \n",
    "LIMIT 10000\n",
    "\"\"\"\n",
    "df_sample = pd.read_sql_query(sample_query, conn)\n",
    "print(f\"Sample shape: {df_sample.shape}\")\n",
    "\n",
    "# 2. SPECIFIC TIME PERIOD (e.g., one month)\n",
    "print(\"\\n=== JANUARY 2020 DATA ===\")\n",
    "january_query = \"\"\"\n",
    "SELECT * FROM hourly_88101_2020 \n",
    "WHERE \"Date GMT\" BETWEEN '2020-01-01' AND '2020-01-31'\n",
    "\"\"\"\n",
    "df_january = pd.read_sql_query(january_query, conn)\n",
    "print(f\"January shape: {df_january.shape}\")\n",
    "\n",
    "# 3. SPECIFIC STATES (e.g., California only)\n",
    "print(\"\\n=== CALIFORNIA DATA ===\")\n",
    "california_query = \"\"\"\n",
    "SELECT * FROM hourly_88101_2020 \n",
    "WHERE \"State Name\" = 'California'\n",
    "\"\"\"\n",
    "df_california = pd.read_sql_query(california_query, conn)\n",
    "print(f\"California shape: {df_california.shape}\")\n",
    "\n",
    "# # 4. HIGH POLLUTION EVENTS ONLY\n",
    "# print(\"\\n=== HIGH POLLUTION EVENTS ===\")\n",
    "# high_pollution_query = \"\"\"\n",
    "# SELECT * FROM hourly_88101_2020 \n",
    "# WHERE \"Sample Measurement\" > 35\n",
    "# ORDER BY \"Sample Measurement\" DESC\n",
    "# \"\"\"\n",
    "# df_high_pollution = pd.read_sql_query(high_pollution_query, conn)\n",
    "# print(f\"High pollution events: {df_high_pollution.shape}\")\n",
    "\n",
    "# # 5. AGGREGATED DATA (fast server-side processing)\n",
    "# print(\"\\n=== STATE-LEVEL AGGREGATIONS ===\")\n",
    "# state_agg_query = \"\"\"\n",
    "# SELECT \n",
    "#     \"State Name\",\n",
    "#     COUNT(*) as total_measurements,\n",
    "#     AVG(\"Sample Measurement\") as avg_pm25,\n",
    "#     MIN(\"Sample Measurement\") as min_pm25,\n",
    "#     MAX(\"Sample Measurement\") as max_pm25,\n",
    "#     COUNT(CASE WHEN \"Sample Measurement\" > 35 THEN 1 END) as high_pollution_days\n",
    "# FROM hourly_88101_2020\n",
    "# GROUP BY \"State Name\"\n",
    "# ORDER BY avg_pm25 DESC\n",
    "# \"\"\"\n",
    "# df_state_stats = pd.read_sql_query(state_agg_query, conn)\n",
    "# print(f\"State aggregations: {df_state_stats.shape}\")\n",
    "# print(df_state_stats.head())\n",
    "\n",
    "# Now you have manageable DataFrames for analysis:\n",
    "print(\"\\n=== AVAILABLE DATAFRAMES ===\")\n",
    "print(f\"df_sample: {df_sample.shape} - Random sample for exploration\")\n",
    "print(f\"df_january: {df_january.shape} - January 2020 data\")\n",
    "print(f\"df_california: {df_california.shape} - California data\")\n",
    "# print(f\"df_high_pollution: {df_high_pollution.shape} - High pollution events\")\n",
    "# print(f\"df_state_stats: {df_state_stats.shape} - State-level statistics\")\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df for california, only site num 0019. Ordered by Date GMT and Time GMT\n",
    "df_ca_0019 = df_california[(df_california['Site Num'] == '0019')]\n",
    "df_ca_0019 = df_ca_0019.sort_values(['Date GMT', 'Time GMT']).reset_index(drop=True)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "df_ca_0019.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Sample Measurement to numeric first, then get value counts ordered by value\n",
    "df_california['Sample Measurement'] = pd.to_numeric(df_california['Sample Measurement'])\n",
    "df_california['Sample Measurement'].value_counts().sort_index()\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_california[df_california['Sample Measurement'] < -10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count distinct 'Uncertainty' values and show the count for each value\n",
    "uncertainty_counts = df_california['Uncertainty'].value_counts().sort_index()\n",
    "print(uncertainty_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df where uncertainty is not null\n",
    "df_uncertainty = df_california[df_california['Uncertainty'].notnull()] \n",
    "#print uncertainty order by uncertainty\n",
    "df_uncertainty = df_uncertainty.sort_values('Uncertainty').reset_index(drop=True)\n",
    "df_uncertainty.tail(100)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot uncertainty histogram\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(df_california['Uncertainty'].dropna(), bins=50, color='skyblue', edgecolor='black')\n",
    "plt.title('Distribution of Uncertainty in California PM2.5 Measurements')\n",
    "plt.xlabel('Uncertainty')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid()\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot uncertainty histogram for \n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(100, 60))\n",
    "plt.hist(df_california['Uncertainty'].dropna(), bins=500, color='skyblue', edgecolor='black')\n",
    "plt.title('Distribution of Uncertainty in California PM2.5 Measurements')\n",
    "plt.xlabel('Uncertainty')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot histograms and boxplots for all columns in df_california\n",
    "for col in df_california.columns:\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    # Try to convert to numeric for plotting, otherwise skip non-numeric columns\n",
    "    try:\n",
    "        data = pd.to_numeric(df_california[col], errors='coerce')\n",
    "        if data.notnull().sum() > 0:\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.hist(data.dropna(), bins=500, color='skyblue', edgecolor='black')\n",
    "            plt.title(f'Histogram of {col}')\n",
    "            plt.xlabel(col)\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.grid()\n",
    "\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.boxplot(data.dropna(), vert=False)\n",
    "            plt.title(f'Boxplot of {col}')\n",
    "            plt.xlabel(col)\n",
    "            plt.grid()\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping column {col}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
